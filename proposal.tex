\documentclass[12pt,letterpaper]{article}
\usepackage{acl2010}

\title{Project Proposal for cs228}
\author{Chinmay Kulkarni and Gabor Angeli}

\begin{document}

\maketitle

\begin{abstract}
This report presents an approach to predicting the helpfulness of a 
	movie review, taking into account features on the content of the article, 
	the sentiments present in the article, and the context of the article
	with respect to previous postings.
Results are compared loosely against the features presented in
	the previous work of 
	\newcite{2006kim-helpfulness}, which most closely resembles this task;
	however, we test on a different domain (IMDB movie
	reviews instead of Amazon product reviews).

We show that we improve upon this baseline, achieving a Pearson correlation
	coefficient of 0.493.
\end{abstract}


\section{Problem Statement}

Online reviews have become popular in number of domains, such as products, restaurants, and movies. Crowdsourced reviews are useful in presenting a diverse range of opinions as well as catering to a wide-range of information needs. 

However, with the increasing number of online reviews comes the related problem of gauging the quality and helpfulness of a review. Several websites now allow their users to mark reviews as helpful or not. Such manual ratings, however, may be sparse. For instance, reviews that have been recently created may have few ratings. Furthermore, a review about a product that already has many reviews would be less frequently seen by visitors, and so less likely to have a manual rating.  

In this project, we focus on the problem of predicting the (manual) helpfulness of a review, based on features extracted from the review text and  helpfulness ratings provided by human annotators for other reviews. 


\subsection{Approach}
Instead of modeling the problem as a straightforward regression of rated helpfulness given review features, we imagine instead a model that considers the manual (observed) rating as a random variable that is influenced by factors that are both {\em intrinsic} and {\em extrinsic} to the review. 

We consider a factor intrinsic if it can be predicted based on the text of a review alone. By extension, an extrinsic factor cannot be predicted purely from the text of a review alone.

In particular, we consider human ratings to be noisy and influenced by {\em extrinsic} factors. For example, reviews written early on are more likely to be rated as helpful, since a reader has fewer other helpful reviews to compare it with (informally, new reviews have a higher bar to pass to be marked as helpful).


\subsection{Applications}
Automatically assigning helpfulness to an online review has possibly many applications. One application is to predict helpfulness of reviews which do not yet have a well-established human-annotated rating (such as newly-created reviews, or reviews for popular products with several other reviews).  Another application is text summarization. In this case, reviews that are more helpful should receive a higher weighting in the summarization task. Current summarization systems tend to consider all reviews as  equally informative {\em a-priori}.

A last application is to assist corporations marketing their products assess which aspects of the 
	product the public likes and dislikes~\cite{2007ghose-helpfulness}. The assumption is that higher rated reviews express more prominent opinions, both positive and negative.



The task of assessing review helpfulness is approached as a regression task:
Given a training corpus of reviews each with a number of helpful/not-helpful
	annotations, predict the helpfulness of a new review.
We approach the task using features based on a set of basic features augmented
	with additional semantic and temporal features, including sentiment
	features.

The approach is tested on a subset of the IMDB corpus of movie reviews.
Although the corpora used in previous work are not available
	to compare against, the system outperforms an informed baseline consisting
	of the features presented in \newcite{2006kim-helpfulness}.


\section{Previous Work}
\subsection{Automatically Assessing Review Helpfulness \cite{2006kim-helpfulness}}
This paper was the earliest work present in the literature on the subject of assessing review helpfulness.
The main idea of the paper was to use SVN regression on a large number of features,
	analyzing the results and the impact of each subset of features on the performance
	of the system.
The paper concluded that a relatively simple feature set (length of the review,
	unigram counts, and star rating) performed the best, with the first two
	(length and unigram counts) alone being almost as effective as all three
	combined.

The dataset used by the paper was a corpus of Amazon reviews, in the categories
	of {\em MP3 Players} and {\em Digital Cameras}.
The dataset consisted of 821 products and 33,016 reviews for {\em MP3 Players},
	and 1,104 products and 26,189 reviews for {\em Digital Cameras}.
This data was then filtered for duplicate or near-duplicate entries ($>80\%$ bigrams match),
	resulting in 85 products and 12,07 reviews being discarded for {\em MP3 Players},
	and 38 products and 3,692 reviews being discarded for {\em Digital Cameras}.
Helpfulness ratings were obtained by taking every review with more than 5 helpfulness
	responses.
This resulted in approximately a third to half of the reviews being discarded.

Training was done using SVM regression.
The authors tested on a variety of kernels, however found a radial bias function (RBF)
	kernel to perform the best; all results are reported using this method.

To assist in feature creation, a {\em Product-Feature} set was automatically extracted.
This was done by mining references to product features from {\tt Epinions.com},
	where users are allowed to describe the pros and cons of each product.
Frequent words were pruned from this list, resulting in around 10,000 unique features
	for both domains.

Features were extracted over each review.
These features fell into the following categories:
\begin{enumerate}
	\item {\bf Structural}: review length ({\tt LEN}); average sentence length, 
		number of sentences, etc. ({\tt SEN}); 
		HTML formatting ({\tt HTM})
	\item {\bf Lexical}: {\it tf-idf} statistic on each unigram ({\tt UGR});
		{\it tf-idf} on each bigram ({\tt BGR})
	\item {\bf Syntactic} ({\tt SYN}): features on POS information
	\item {\bf Semantic}: product features ({\tt PRF}); 
		{\it General Inquirer} sentiment words ({\tt GIW}) 
	\item {\bf Meta Info}: star rating (average/deviation) ({\tt STR})
\end{enumerate}

Evaluation was done using the Spearman correlation coefficient.
The best results came from the three features ({\tt LEN+UGR+STR}),
	resulting in a Spearman coefficient of 0.656 on {\em MP3 Players}
	and 0.595 on {\em Digital Cameras}.
Adding additional features tended to hurt performance; adding every feature
	dropped the {\em MP3 Player} score to 0.601,
	although mildly improving the {\em Digital Camera} score to 0.604.

\subsection{A Joint Model of Text and Aspect Ratings for Sentiment Summarization \cite{2008titov-summarization}}

This paper presents an approach to summarization, jointly learning the topics to summarize and text which
	describes them.
The paper proposes a model ({\em Multi-Aspect Sentiment Model}) consisting of two parts: 
	an unsupervised topic model, and a classifier from words to sentiment ratings.
The paper evaluates on a hotel review dataset taken from {\tt TripAdvisor.com}.

The paper presents the task of summarization as a two-fold task: the first task
	is described as {\em aspect identification and mention extraction} -- determining which aspects of the
	reviews are relevant to describe, and determining which text fragments describe them.
The second task is {\em sentiment classification} -- determining the sentiment on the
	relevant extracted text.
The paper attempts to incorporate both of these tasks into a single model which extracts text fragments
	and their associated rating, given the review and a per-aspect rating.

The dataset used in the paper consists of 10,000 reviews from {\tt TripAdvisor.com}, where each
review was rated in at least {\em service}, {\em location}, and {\em rooms}.

The approach taken is the build a model -- coined as a Multi-Aspect Sentiment model (MAS) --
	which is effectively built on a combination of a multi-grain LDA topic model and a series of MaxEnt
	classifiers for each topic.
The model is such that a word in the document is sampled from either a local or global topic;
	the intent is that global topics will capture topics corresponding to non-sentiment phenomena
	(e.g. {\em MP3 players} versus {\em hotels}), while the local topics will capture sentiment-laden
	words.

The paper raises the issue that often aspects of reviews correlate strongly with each other.
That is, if you dislike a hotel, you will likely not rate any aspect highly.
To address this, the model classifies not over absolute ratings, but over the difference between
	the aspect rating and the overall rating.

Inference over the model was done using Gibbs sampling, as exact inference is intractable.
The model achieves a precision of between 75\% and 85\% for the different aspects.

\bibliographystyle{acl}
\bibliography{main}
\end{document}

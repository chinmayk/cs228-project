\documentclass[12pt,letterpaper]{article}
\include{std-macros}
\usepackage{acl2010}
\usepackage{graphicx}

\title{Project Proposal for cs228}
\author{Chinmay Kulkarni and Gabor Angeli}

\begin{document}

\maketitle

\section{Problem Statement}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% IMPLEMENTATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Section{implementation}{Implementation}
\subsection{Model}
\FigStar{model}{0.43}{model}{
	The graphical model proposed to be used in the project.
	Grey nodes denote observed features; the blue node denotes the
	variable that is being tested; the red node denotes a feature 
	derived from a topic model.
}
To solve the problem, we intend to employ a directed graphical model.
The model is shown in \ref{modelFig}.
Observed variables are colored grey. 
The Feature variables (i.e. {\em Lexical Features}, {\em Structural Features}, 
	etc) are compressed into a single node for illustrative purposes;
	these nodes would either be split into individual features.
The variable we are interested in ({\em Extrinsic Helpfulness \%}) is marked
	in blue.
A node denoting the number of new topics introduced in the review is marked
	in red, as it is a placeholder for an embedded topic model.

The essense of the model is to represent the two hypotheses presented in the
	introduction. 
The decoupling of observations into {\em extrinsic} and
	{\em intrinsic} influences is done by placing the nodes either at the top
	of the graph (influencing {\em Observed Helpfulness \%}), or at the bottom
	(a result of {\em Intrinsic Helpfulness \%}).
Observed helpfulness is in turn affected by a notion of intrinsic helpfulness
	in conjunction with the noise factors.
The features used for this part of the model align roughly with those of
	\newcite{2006kim-helpfulness}.

The second hypthesis is modeled by the red node {\em Number of New Topics}.
This will be an implementation of LDA, either trained separately from or
	jointly with the rest of the model.
The paper by \newcite{2008titov-summarization} attempts a similar technique
	for review summarization.

A directed model was chosen since the intuition of the problem appears to
	align well with a directed model paradigm.
For instance, {\em True Helpfulness} is a factor in causing the value of
	{\em Observed Helpfulness}, in conjunction with the other extrinsic
	causes.
Similarly, {\em New Topics} causes a higher helpfulness, etc.

\subsection{Training}
The problem as presented is a supervised training problem.
Pairs of reviews and their associated observed helpfulness are provided at
	training time.
At test time, the task is to predict the observed helpfulness from the review
	and other reviews from the same item.

The data used for these experiments is taken from the IMDB corpus, which
	consists of a total of 45,772 movies and 1,808,564 reviews.
In all likelihood only a small subset of the corpus will be used.
We have access to this corpus.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PRELIMINARY RESEARCH
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Section{preliminary}{Preliminary Research}

This project builds off of a project done for cs424 last quarter.
That project consisted of training a simple classifier with features
	defined both over the {\em extrinsic} and {\em intrinsic} features.
The results of the project were reasonable (pearson correlation of 0.486)
	though by no means good.
Furthermore, the most useful features in the model were the ones that
	effectively modeled the noise in the rating process, suggesting the
	feasibility of {\bf Hyp1}.

Statistics were collected on the dataset to support the two hypotheses.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PREVIOUS WORK
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Previous Work}
This section details some of the previous work done in the field.
The first briefly describes one of the first and most prototypical papers
	on assessing review helpfulness.
The second presents an approach to jointly performing text summarization
	and topic modeling; this approach appears relevant to our task.

\subsection{Automatically Assessing Review Helpfulness \cite{2006kim-helpfulness}}
This paper was the earliest work present in the literature on the subject of assessing review helpfulness.
The main idea of the paper was to use SVN regression on a large number of features,
	analyzing the results and the impact of each subset of features on the performance
	of the system.
The paper concluded that a relatively simple feature set (length of the review,
	unigram counts, and star rating) performed the best, with the first two
	(length and unigram counts) alone being almost as effective as all three
	combined.

The dataset used by the paper was a corpus of Amazon reviews, in the categories
	of {\em MP3 Players} and {\em Digital Cameras}.
The dataset consisted of 821 products and 33,016 reviews for {\em MP3 Players},
	and 1,104 products and 26,189 reviews for {\em Digital Cameras}.
This data was then filtered for duplicate or near-duplicate entries ($>80\%$ bigrams match),
	resulting in 85 products and 12,07 reviews being discarded for {\em MP3 Players},
	and 38 products and 3,692 reviews being discarded for {\em Digital Cameras}.
Helpfulness ratings were obtained by taking every review with more than 5 helpfulness
	responses.
This resulted in approximately a third to half of the reviews being discarded.

Training was done using SVM regression.
The authors tested on a variety of kernels, however found a radial bias function (RBF)
	kernel to perform the best; all results are reported using this method.

To assist in feature creation, a {\em Product-Feature} set was automatically extracted.
This was done by mining references to product features from {\tt Epinions.com},
	where users are allowed to describe the pros and cons of each product.
Frequent words were pruned from this list, resulting in around 10,000 unique features
	for both domains.

Features were extracted over each review.
These features fell into the following categories:
\begin{enumerate}
	\item {\bf Structural}: review length ({\tt LEN}); average sentence length, 
		number of sentences, etc. ({\tt SEN}); 
		HTML formatting ({\tt HTM})
	\item {\bf Lexical}: {\it tf-idf} statistic on each unigram ({\tt UGR});
		{\it tf-idf} on each bigram ({\tt BGR})
	\item {\bf Syntactic} ({\tt SYN}): features on POS information
	\item {\bf Semantic}: product features ({\tt PRF}); 
		{\it General Inquirer} sentiment words ({\tt GIW}) 
	\item {\bf Meta Info}: star rating (average/deviation) ({\tt STR})
\end{enumerate}

Evaluation was done using the Spearman correlation coefficient.
The best results came from the three features ({\tt LEN+UGR+STR}),
	resulting in a Spearman coefficient of 0.656 on {\em MP3 Players}
	and 0.595 on {\em Digital Cameras}.
Adding additional features tended to hurt performance; adding every feature
	dropped the {\em MP3 Player} score to 0.601,
	although mildly improving the {\em Digital Camera} score to 0.604.

\subsection{A Joint Model of Text and Aspect Ratings for Sentiment Summarization \cite{2008titov-summarization}}

This paper presents an approach to summarization, jointly learning the topics to summarize and text which
	describes them.
The paper proposes a model ({\em Multi-Aspect Sentiment Model}) consisting of two parts: 
	an unsupervised topic model, and a classifier from words to sentiment ratings.
The paper evaluates on a hotel review dataset taken from {\tt TripAdvisor.com}.

The paper presents the task of summarization as a two-fold task: the first task
	is described as {\em aspect identification and mention extraction} -- determining which aspects of the
	reviews are relevant to describe, and determining which text fragments describe them.
The second task is {\em sentiment classification} -- determining the sentiment on the
	relevant extracted text.
The paper attempts to incorporate both of these tasks into a single model which extracts text fragments
	and their associated rating, given the review and a per-aspect rating.

The dataset used in the paper consists of 10,000 reviews from {\tt TripAdvisor.com}, where each
review was rated in at least {\em service}, {\em location}, and {\em rooms}.

The approach taken is the build a model -- coined as a Multi-Aspect Sentiment model (MAS) --
	which is effectively built on a combination of a multi-grain LDA topic model and a series of MaxEnt
	classifiers for each topic.
The model is such that a word in the document is sampled from either a local or global topic;
	the intent is that global topics will capture topics corresponding to non-sentiment phenomena
	(e.g. {\em MP3 players} versus {\em hotels}), while the local topics will capture sentiment-laden
	words.

The paper raises the issue that often aspects of reviews correlate strongly with each other.
That is, if you dislike a hotel, you will likely not rate any aspect highly.
To address this, the model classifies not over absolute ratings, but over the difference between
	the aspect rating and the overall rating.

Inference over the model was done using Gibbs sampling, as exact inference is intractable.
The model achieves a precision of between 75\% and 85\% for the different aspects.

\bibliographystyle{acl}
\bibliography{main}
\end{document}
